{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning\n",
    "## Contents\n",
    "1. [Dealing with missing values](http://localhost:8888/notebooks/Documents/GitHub/Kaggle/home-data-for-ml-course/Intermediate%20Machine%20Learning.ipynb#Dealing-with-missing-values)\n",
    "2. [Dealing with categorical values](http://localhost:8888/notebooks/Documents/GitHub/Kaggle/home-data-for-ml-course/Intermediate%20Machine%20Learning.ipynb#Dealing-with-categorical-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading train and test data\n",
    "We use the data from home-data-for-ml-course from Kaggle:\n",
    "* train.csv, which we split into train and validation datasets\n",
    "* test.csv, which we use to test our model\n",
    "\n",
    "We preliminarily clean the data, by:\n",
    "* dropping the rows with a null ('Nan') target variable\n",
    "* separating the target variable (to y) and the predictor variables (to X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.read_csv('train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('test.csv', index_col='Id') # has no target column ('SalePrice')\n",
    "\n",
    "# Drop rows (axis=0) without the target predictor, where SalePrice = NaN\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "# Separate the target variable (to the variable y), then remove target column (axis=1) from the predictor variables\n",
    "y = X_full.SalePrice\n",
    "X = X_full.drop(axis=1, columns=['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of exploring how to deal with missing values, we define X and X_test to exclude categorical predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep things simple, only use numerical predictors\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Separate data into train and validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing values\n",
    "\n",
    "3 methods:\n",
    "\n",
    "1. Dropping columns\n",
    "2. Imputing columns (strategy='mean', 'median', 'most_frequent' or 'constant') - see https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "3. Imputing columns and adding a col_was_missing column with DataFrame.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminarily investigating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>20</td>\n",
       "      <td>90.0</td>\n",
       "      <td>11694</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "      <td>452.0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6600</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>80.0</td>\n",
       "      <td>13360</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1921</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13265</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>857</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>20</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13704</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>843</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>20</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2004</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>84.0</td>\n",
       "      <td>8658</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>101.0</td>\n",
       "      <td>643</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>440</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>160</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2572</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>604</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>180</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>297</td>\n",
       "      <td>120</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1941</td>\n",
       "      <td>1950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "Id                                                                            \n",
       "619           20         90.0    11694            9            5       2007   \n",
       "871           20         60.0     6600            5            5       1962   \n",
       "93            30         80.0    13360            5            7       1921   \n",
       "818           20          NaN    13265            8            5       2002   \n",
       "303           20        118.0    13704            7            5       2001   \n",
       "1455          20         62.0     7500            7            5       2004   \n",
       "41            20         84.0     8658            6            5       1965   \n",
       "960          160         24.0     2572            7            5       1999   \n",
       "76           180         21.0     1596            4            5       1973   \n",
       "1390          50         60.0     6000            6            6       1941   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\n",
       "Id                                                      ...               \n",
       "619           2007       452.0          48           0  ...         774   \n",
       "871           1962         0.0           0           0  ...         308   \n",
       "93            2006         0.0         713           0  ...         432   \n",
       "818           2002       148.0        1218           0  ...         857   \n",
       "303           2002       150.0           0           0  ...         843   \n",
       "1455          2005         0.0         410           0  ...         400   \n",
       "41            1965       101.0         643           0  ...         440   \n",
       "960           1999         0.0         604           0  ...         484   \n",
       "76            1973         0.0         462           0  ...         297   \n",
       "1390          1950         0.0         375           0  ...         440   \n",
       "\n",
       "      WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  \\\n",
       "Id                                                                     \n",
       "619            0          108              0          0          260   \n",
       "871            0            0              0          0            0   \n",
       "93             0            0             44          0            0   \n",
       "818          150           59              0          0            0   \n",
       "303          468           81              0          0            0   \n",
       "1455           0          113              0          0            0   \n",
       "41             0          138              0          0            0   \n",
       "960            0           44              0          0            0   \n",
       "76           120          101              0          0            0   \n",
       "1390           0            0              0          0            0   \n",
       "\n",
       "      PoolArea  MiscVal  MoSold  YrSold  \n",
       "Id                                       \n",
       "619          0        0       7    2007  \n",
       "871          0        0       8    2009  \n",
       "93           0        0       8    2009  \n",
       "818          0        0       7    2008  \n",
       "303          0        0       1    2006  \n",
       "1455         0        0      10    2009  \n",
       "41           0        0      12    2006  \n",
       "960          0        0       5    2010  \n",
       "76           0        0      11    2009  \n",
       "1390         0        0       3    2007  \n",
       "\n",
       "[10 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are some columns with missing values\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset: (1168, 36)\n",
      "Shape of validation dataset: (292, 36)\n"
     ]
    }
   ],
   "source": [
    "# Gives (rows, columns) of the training and validation datasets\n",
    "print('Shape of training dataset:', X_train.shape)\n",
    "print('Shape of validation dataset:', X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotFrontage    212\n",
      "MasVnrArea       6\n",
      "GarageYrBlt     58\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identifies number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n"
     ]
    }
   ],
   "source": [
    "# Creates a list ('cols_with_missing') containing the headers of columns with missing values\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "print(cols_with_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to score datasets\n",
    "* In this notebook, we use the 3 methods above (dropping columns, imputation, and imputation with adding a _was_dropped column)\n",
    "* The 3 methods modify the X_train and X_valid datasets\n",
    "* We score the ability of the 3 modified X_train datasets to train a model; against its performance in the modified X_valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_valid)\n",
    "    return mean_absolute_error(pred, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Dropping columns\n",
    "Use the method to drop columns with missing values, i.e. NaN or None.\n",
    "    \n",
    "    [DataFrameName].drop(cols_with_missing, axis=1)\n",
    "    \n",
    "This needs to be applied to both the X_train and X_valid datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset, after dropping columns: (1168, 33)\n",
      "Shape of validation dataset, after dropping columns: (292, 33)\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the training and validation datasets\n",
    "X_train_drop = X_train.copy()\n",
    "X_valid_drop = X_valid.copy()\n",
    "\n",
    "# Drop the columns in cols_with_missing\n",
    "X_train_drop.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_drop.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "print('Shape of training dataset, after dropping columns:', X_train_drop.shape)\n",
    "print('Shape of validation dataset, after dropping columns:', X_valid_drop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Imputation\n",
    "Imputation refers to replacing NaN values with the:\n",
    "* the mean (strategy='mean')\n",
    "* the median (strategy='median')\n",
    "* the mode (strategy='most_frequent')\n",
    "* a constant value (strategy='constant', fill_value=k)\n",
    "\n",
    "See \n",
    "\n",
    "    help(SimpleImputer())\n",
    "\n",
    "for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "\n",
      "Shape of training dataset, after imputing values: (1168, 36)\n",
      "Shape of validation dataset, after imputing values: (292, 36)\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the training and validation datasets\n",
    "X_train_imput1 = X_train.copy()\n",
    "X_valid_imput1 = X_valid.copy()\n",
    "\n",
    "# Check whether there are any missing values in X_train and X_valid\n",
    "print(X_train.isnull().any().any())\n",
    "print(X_valid.isnull().any().any())\n",
    "\n",
    "# Create an instance of SimpleImputer\n",
    "my_imputer_1 = SimpleImputer()\n",
    "\n",
    "X_train_imput1 = pd.DataFrame(my_imputer_1.fit_transform(X_train_imput1))\n",
    "X_valid_imput1 = pd.DataFrame(my_imputer_1.transform(X_valid_imput1))\n",
    "\n",
    "# Check whether there are an missing values in X_train and X_valid\n",
    "print(X_train_imput1.isnull().any().any())\n",
    "print(X_valid_imput1.isnull().any().any())\n",
    "print()\n",
    "print('Shape of training dataset, after imputing values:', X_train_imput1.shape)\n",
    "print('Shape of validation dataset, after imputing values:', X_valid_imput1.shape)\n",
    "\n",
    "# Imputation removes column headings, add them back\n",
    "X_train_imput1.columns = X_valid.columns\n",
    "X_valid_imput1.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Imputation redux (adding another column)\n",
    "To help the model distinguish between actual and imputed values, for each column that has imputed values, we add another column ('ColName_was_missing') that contains X_train[ColName].isnull() or X_valid[ColName].isnull()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "\n",
      "Shape of training dataset, after imputing values and adding columns: (1168, 39)\n",
      "Shape of validation dataset, after imputing values and adding columns: (292, 39)\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the training and validation datasets\n",
    "X_train_imput2 = X_train.copy()\n",
    "X_valid_imput2 = X_valid.copy()\n",
    "\n",
    "# Add columns to label rows with imputed values\n",
    "for col in cols_with_missing:\n",
    "    X_train_imput2[col + '_was_missing'] = X_train_imput2[col].isnull()\n",
    "    X_valid_imput2[col + '_was_missing'] = X_valid_imput2[col].isnull()\n",
    "    \n",
    "# Inspect dataframe to see that rows are added correctly\n",
    "# X_train_imput2.head()\n",
    "# X_valid_imput2.head()\n",
    "\n",
    "# Impute missing values, as before. Another instance of SimpleImputer() is created again, since it needs to be retrained\n",
    "my_imputer_2 = SimpleImputer()\n",
    "X_train_imput2 = pd.DataFrame(my_imputer_2.fit_transform(X_train_imput2))\n",
    "X_valid_imput2 = pd.DataFrame(my_imputer_2.transform(X_valid_imput2))\n",
    "\n",
    "# Check whether there are an missing values in X_train and X_valid\n",
    "print(X_train_imput2.isnull().any().any())\n",
    "print(X_valid_imput2.isnull().any().any())\n",
    "print()\n",
    "print('Shape of training dataset, after imputing values and adding columns:', X_train_imput2.shape)\n",
    "print('Shape of validation dataset, after imputing values and adding columns:', X_valid_imput2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the interventions of the model\n",
    "* As defined in the score_dataset() function above, the model is a RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1 - drop columns: 17837.83\n",
      "Method 2 - impute values: 18062.89\n",
      "Method 3 - impute values, add isnull column: 18148.42\n"
     ]
    }
   ],
   "source": [
    "# help(RandomForestRegressor)\n",
    "\n",
    "# In this case, score_dataset(X_train, X_valid, y_train, y_valid) would return an error.\n",
    "\n",
    "print('Method 1 - drop columns:', round(score_dataset(X_train_drop, X_valid_drop, y_train, y_valid), 2))\n",
    "print('Method 2 - impute values:', round(score_dataset(X_train_imput1, X_valid_imput1, y_train, y_valid), 2))\n",
    "print('Method 3 - impute values, add isnull column:', round(score_dataset(X_train_imput2, X_valid_imput2, y_train, y_valid), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with categorical values\n",
    "3 methods:\n",
    "1. Dropping categorical values\n",
    "2. Assigning a value (only works for ordinal categories, like Likert scale <==> [-2, -1, 0, 1, 2]\n",
    "3. One-hot encoding (but doesn't work if there are too many categories for each predictor variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some pre-processing\n",
    "We need to respecify X and X_test, since we'd excluded categorical predictor variables earlier.\n",
    "* Redefine X and X_test from X_full and X_test_full, including categorical predictor variables\n",
    "* Call train_test_split on X, y again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "# Call train_test_split on X, y\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, train_size = 0.8, test_size = 0.2, random_state=0)\n",
    "\n",
    "# Remove columns with NaN values\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]\n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "# print(list(X_valid_full.columns))\n",
    "\n",
    "# Select categorical columns with low cardinality\n",
    "low_cardinality_cols = [col for col in X_train_full.columns \n",
    "                        if X_train_full[col].dtypes == 'object' and X_train_full[col].nunique() < 10\n",
    "                       ]\n",
    "# print(low_cardinality_cols)\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = [col for col in X_train_full.columns if X_train_full[col].dtypes in ['int64', 'float64']]\n",
    "# print(num_cols)\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + num_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Dropping categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the training and validation datasets\n",
    "drop_X_train = X_train.copy()\n",
    "drop_X_valid = X_valid.copy()\n",
    "\n",
    "# Drop all categorical variables from the datasets\n",
    "drop_X_train = drop_X_train.select_dtypes(exclude='object')\n",
    "drop_X_valid = drop_X_valid.select_dtypes(exclude='object')\n",
    "\n",
    "# Run a check on dtypes of drop_X_train and drop_X_valid\n",
    "# drop_X_train.dtypes == 'object'\n",
    "# drop_X_valid.dtypes == 'object'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Assigning a numerical value to each categorical class\n",
    "The main challenge is that some categorical classes may only be present in the validation dataset and not in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tar&Grv', 'Metal', 'WdShngl', 'Membran', 'CompShg', 'Roll', 'WdShake'}\n",
      "{'Tar&Grv', 'ClyTile', 'WdShngl', 'CompShg'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are classes in one dataset that are not present in the other\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtypes == 'object']\n",
    "print(set(X_train[object_cols[12]]))\n",
    "print(set(X_valid[object_cols[12]]))\n",
    "set(X_train[X_train.columns[15]]) == set(X_valid[X_train.columns[15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "\n",
      " ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "# Identifying all categorical values\n",
    "# X_train.dtypes is a dataframe with the column headers as the index (left), and the dtype as the other column (right)\n",
    "# X_train.dtypes == 'object' is a dataframe with the column headers as the index (left), and a bool as the other column (right)\n",
    "s = (X_train.dtypes == 'object')\n",
    "\n",
    "# s[s] is equal to s[s == True]; returns the second dataframe, but only for values where the bool == True\n",
    "# print(s[s])\n",
    "# s[s].index returns an Index() of all indexes in s[s]\n",
    "object_cols = list(s[s].index)\n",
    "print('Categorical variables:\\n\\n', object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical labels that will be label encoded:\n",
      " ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'BldgType', 'HouseStyle', 'ExterQual', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleCondition']\n",
      "\n",
      "Cagegorical labels that will be dropped:\n",
      " ['RoofMatl', 'ExterCond', 'Foundation', 'HeatingQC', 'Functional', 'Condition1', 'Heating', 'RoofStyle', 'LandSlope', 'Condition2', 'SaleType', 'Utilities']\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the training and validation datasets\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Only select categorical variables where the classes in the training dataset == classes in the validation dataset\n",
    "good_label_cols = [col for col in object_cols if set(X_valid[col]) == set(X_train[col])]\n",
    "\n",
    "# Select problematic columns to be removed\n",
    "bad_label_cols = list(set(object_cols) - set(good_label_cols))\n",
    "\n",
    "print('Categorical labels that will be label encoded:\\n', good_label_cols)\n",
    "print('\\nCagegorical labels that will be dropped:\\n', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop problematic columns from X_train and X_valid\n",
    "label_X_train.drop(bad_label_cols, axis=1, inplace=True)\n",
    "label_X_valid.drop(bad_label_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in good_label_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(label_X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(label_X_valid[col])\n",
    "    \n",
    "# Check that there are no dtypes = 'object'\n",
    "# s = label_X_train.dtypes == 'object'\n",
    "# s.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Street', 2),\n",
       " ('Utilities', 2),\n",
       " ('CentralAir', 2),\n",
       " ('LandSlope', 3),\n",
       " ('PavedDrive', 3),\n",
       " ('LotShape', 4),\n",
       " ('LandContour', 4),\n",
       " ('ExterQual', 4),\n",
       " ('KitchenQual', 4),\n",
       " ('MSZoning', 5),\n",
       " ('LotConfig', 5),\n",
       " ('BldgType', 5),\n",
       " ('ExterCond', 5),\n",
       " ('HeatingQC', 5),\n",
       " ('Condition2', 6),\n",
       " ('RoofStyle', 6),\n",
       " ('Foundation', 6),\n",
       " ('Heating', 6),\n",
       " ('Functional', 6),\n",
       " ('SaleCondition', 6),\n",
       " ('RoofMatl', 7),\n",
       " ('HouseStyle', 8),\n",
       " ('Condition1', 9),\n",
       " ('SaleType', 9)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col : X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x : x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of training and validation datasets\n",
    "OH_X_train = X_train.copy()\n",
    "OH_X_valid = X_valid.copy()\n",
    "\n",
    "# Creating an instance of OneHotEncoder\n",
    "OH_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# One-hot encoding\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(OH_X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(OH_X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removes indexes; add them back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns from OH_X_train, OH_X_valid\n",
    "OH_X_train.drop(object_cols, axis=1, inplace=True)\n",
    "OH_X_valid.drop(object_cols, axis=1, inplace=True)\n",
    "\n",
    "# Combine OH_X_train/valid with OH_cols_train/valid\n",
    "OH_X_train = pd.concat([OH_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([OH_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# None of the remaining dtypes are objects\n",
    "# s = OH_X_train.dtypes == 'object'\n",
    "# s[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the interventions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with categorical variables):\n",
      "17837.82570776256\n",
      "MAE from Approach 2 (Label Encoding):\n",
      "17588.240936073056\n",
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "17525.345719178084\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE from Approach 1 (Drop columns with categorical variables):\") \n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
